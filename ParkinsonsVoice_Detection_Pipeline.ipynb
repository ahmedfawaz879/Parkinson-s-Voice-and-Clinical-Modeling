{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "cd49c6b6-7f21-4d9a-ad25-21d259358686",
      "cell_type": "markdown",
      "source": "Parkinson's Progression Markers Initiative (PPMI) —\nPreprocessing & Analysis Starter Notebook\nPurpose:\n- Ready-to-run Jupyter notebook scaffold to prepare PPMI Clinical data for downstream\nmodeling and analysis. Designed to work with CSV downloads from the PPMI portal.\nUsage:\n1. Place your downloaded PPMI CSV files into `./PPMI_raw/` (or change DATA_DIR below).\n2. Run the cells in order. Cells that depend on files you don't yet have will warn but\nwill not crash.\n loader cells to continue ML pipeline.\n\n Notebook author: Ahmed Fawaz\n\n",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      }
    },
    {
      "id": "24bc3acc-b9bf-4aee-b76d-a5a6a80763b9",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a87cca8b-361e-4d72-9cf9-e011ac4857ec",
      "cell_type": "code",
      "source": "# -------------------------\n# 0. Environment & Settings\n# -------------------------\nimport os\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\n\n\n# Visual settings\nsns.set(style=\"whitegrid\")\npd.set_option(\"display.max_columns\", 120)\n\n\n# Logging\nlogging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n\n\n# Paths (change to suit your environment)\nDATA_DIR = os.getenv('PPMI_RAW_DIR', './PPMI_raw')\nPROCESSED_DIR = os.getenv('PPMI_PROCESSED_DIR', './processed')\nOUTPUT_DIR = os.getenv('PPMI_OUTPUT_DIR', './output')\nos.makedirs(DATA_DIR, exist_ok=True)\nos.makedirs(PROCESSED_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\nprint(f\"DATA_DIR = {DATA_DIR}\")\nprint(f\"PROCESSED_DIR = {PROCESSED_DIR}\")\nprint(f\"OUTPUT_DIR = {OUTPUT_DIR}\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e748c4c8-d8f0-4ca0-be24-e726f8069fb2",
      "cell_type": "markdown",
      "source": "Quick cell: install missing packages (run only if necessary)\n\nIf you're running in a fresh environment, uncomment & run the next cell to install packages.\n\n\n\n!pip install pandas matplotlib seaborn scikit-learn imbalanced-learn shap lime",
      "metadata": {}
    },
    {
      "id": "aa3d155b-5e40-4428-9abb-59f9d0bdddfb",
      "cell_type": "code",
      "source": "# ## 1. Helper functions to discover and load PPMI CSVs\n# The PPMI download contains many CSVs whose filenames may include a date suffix.\n# These helpers find files by partial name match and read them robustly.\n\n\n\n\n\ndef find_file(partial_name, directory=DATA_DIR):\n\"\"\"Return the first matching CSV for partial_name in directory, else None.\"\"\"\npattern = os.path.join(directory, f\"*{partial_name}*.csv\")\nmatches = sorted(glob(pattern))\nif not matches:\nlogging.warning(f\"No file matches for '{partial_name}' in {directory}\")\nreturn None\nif len(matches) > 1:\nlogging.info(f\"Multiple matches for '{partial_name}', using first: {matches[0]}\")\nreturn matches[0]\n\n\n\n\ndef load_csv_by_partial(partial_name, directory=DATA_DIR, dtype=None, parse_dates=None):\n\"\"\"Load a CSV matched by partial_name, with optional dtype/parse_dates.\n\n\nReturns: DataFrame or None (if file missing)\n\"\"\"\nfp = find_file(partial_name, directory)\nif fp is None:\nreturn None\ntry:\ndf = pd.read_csv(fp, dtype=dtype, parse_dates=parse_dates, low_memory=False)\nlogging.info(f\"Loaded '{partial_name}' -> {os.path.basename(fp)} ({df.shape})\")\nreturn df\nexcept Exception as e:\nlogging.error(f\"Failed to load {fp}: {e}\")\nreturn None\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eaf81082-1b4f-49c1-942c-23c5aa74e260",
      "cell_type": "code",
      "source": "# ## 2. Load core PPMI tables (safe to run now; will warn if files missing)\n# We'll attempt to load the most commonly used clinical tables referenced in the\n# PPMI Data User Guide. If you only have a subset, proceed with what you have.\n\n\n\ncore_tables = {\n'Participant_Status': None,\n'Demographics': None,\n'MDS-UPDRS_Part_I': None,\n'MDS-UPDRS_Part_I_Patient_Questionnaire': None,\n'MDS-UPDRS_Part_II__Patient_Questionnaire': None,\n'MDS-UPDRS_Part_III': None,\n'LEDD_Concomitant_Medication_Log': None,\n'Concomitant_Medication_Log': None,\n'UPSIT': None,\n'Codes': None,\n}\n\n\nfor name in list(core_tables.keys()):\ncore_tables[name] = load_csv_by_partial(name)\n\n\n# Make variables for convenience (will be None if not present)\nParticipant_Status = core_tables['Participant_Status']\nDemographics = core_tables['Demographics']\nMDS_UPDRS_I = core_tables['MDS-UPDRS_Part_I']\nMDS_UPDRS_I_P = core_tables['MDS-UPDRS_Part_I_Patient_Questionnaire']\nMDS_UPDRS_II = core_tables['MDS-UPDRS_Part_II__Patient_Questionnaire']\nMDS_UPDRS_III = core_tables['MDS-UPDRS_Part_III']\nLEDD_log = core_tables['LEDD_Concomitant_Medication_Log']\nConcomitant_Med = core_tables['Concomitant_Medication_Log']\nCodes = core_tables['Codes']\nUPSIT = core_tables['UPSIT']\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c7db374f-bf45-4423-930e-07e2bf90ba3f",
      "cell_type": "code",
      "source": "# ## 3. Basic inspection utilities\n# Small helpers to inspect tables and common issues found in PPMI data (case variations,\n# date parsing needs, missing PATNOs, etc.)\n\n\n\n\ndef show_basic_info(df, name, n=5):\nif df is None:\nlogging.warning(f\"{name} is not loaded\")\nreturn\nprint(f\"\\n--- {name} ({df.shape}) ---\")\ndisplay(df.head(n))\nprint('\\nColumn dtypes:')\nprint(df.dtypes)\nprint('\\nMissing values (top 10):')\nprint(df.isnull().sum().sort_values(ascending=False).head(10))\n\n\n\n\n# Example usage (safe to run)\nshow_basic_info(Participant_Status, 'Participant_Status')\nshow_basic_info(Demographics, 'Demographics')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4d2e207-ad05-4d47-ad18-3b01f8c825be",
      "cell_type": "code",
      "source": "# ## 4. Build Participant Master table (static participant-level info)\n# Follow guide: keep participants with ENROLL_STATUS in (enrolled, withdrew, complete)\n\n\n\n\n\ndef build_participant_master(participant_status_df, demographics_df, codes_df=None):\n\"\"\"Return a master table with selected participants and decoded fields when possible.\"\"\"\nif participant_status_df is None:\nlogging.error(\"Participant_Status table is required to build master table\")\nreturn None\nps = participant_status_df.copy()\n\n\n# Normalize enroll status (lowercase) to handle mixed case\nif 'ENROLL_STATUS' in ps.columns:\nps['ENROLL_STATUS'] = ps['ENROLL_STATUS'].astype(str).str.lower()\nelse:\nlogging.warning('ENROLL_STATUS missing; proceeding without filtering by status')\n\n\nvalid_status = ['enrolled', 'withdrew', 'complete']\nif 'ENROLL_STATUS' in ps.columns:\nps = ps[ps['ENROLL_STATUS'].isin(valid_status)].copy()\n\n\n# Merge demographics\nif demographics_df is not None:\nmerged = ps.merge(demographics_df, on='PATNO', how='left', suffixes=('', '_demo'))\nelse:\nmerged = ps\n\n\n# Optionally decode codes with Codes table\n# (Codes table has mappings for many coded fields; user can expand as needed)\n\n\nreturn merged\n\n\nparticipant_master = build_participant_master(Participant_Status, Demographics, Codes)\nshow_basic_info(participant_master, 'Participant_Master')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d549758-c1d4-4611-80ea-8c089d4cd830",
      "cell_type": "code",
      "source": "# ## 5. Cohort composition & quick stats (for data access proposal / EDA)\n\n\n\n\ndef cohort_summary(participant_status_df):\nif participant_status_df is None:\nlogging.warning('Participant_Status not available')\nreturn\nps = participant_status_df.copy()\nps['ENROLL_STATUS'] = ps['ENROLL_STATUS'].astype(str).str.lower()\ndisplay(ps.groupby(['COHORT_DEFINITION', 'ENROLL_STATUS']).size().unstack(fill_value=0))\n\n\ncohort_summary(Participant_Status)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e7952329-af22-477c-971f-324295bc2c2c",
      "cell_type": "code",
      "source": "# ## 6. Preparing longitudinal UPDRS summary (example)\n# We create a per-visit total for each participant for Part I / II / III where available.\n# The user guide notes that part III may have special '101' values (unable to rate).\n\n\n\n\n\ndef compute_updrs_total(df_part, part_prefix='NP'):\n\"\"\"Attempt to compute a total score across typical NP* columns. Designed to be robust\nto variation in column names. Returns df with a TOTAL column.\n\"\"\"\nif df_part is None:\nlogging.warning('UPDRS table not provided')\nreturn None\ndf = df_part.copy()\n\n\n# normalize INFODT if present\nif 'INFODT' in df.columns:\ndf['INFODT'] = pd.to_datetime(df['INFODT'], errors='coerce')\n\n\n# Identify score columns: common pattern in guide is NP* or similar\nscore_cols = [c for c in df.columns if (c.startswith('NP') or c.startswith('MDS') or c.upper().startswith('P'))]\n# Fallback: numeric columns except identifiers / dates\nif not score_cols:\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\npossible = [c for c in numeric_cols if c not in ['PATNO']]\nscore_cols = possible\n\n\nif not score_cols:\nlogging.warning('No candidate score columns found to compute TOTAL')\nreturn df\n\n\n# Handle special '101' -> NaN for part III 'unable to rate' as per guide\ndf[score_cols] = df[score_cols].apply(pd.to_numeric, errors='coerce')\ndf['TOTAL'] = df[score_cols].sum(axis=1, skipna=True)\n\n\nreturn df\n\n\nupdrs1_tot = compute_updrs_total(MDS_UPDRS_I)\nshow_basic_info(updrs1_tot, 'UPDRS Part I (with TOTAL)')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a169fe69-96ce-4167-a22d-419c7c93b375",
      "cell_type": "code",
      "source": "# ## 7. Visual example: UPDRS progression for a sample participant\n\n\n\n\ndef plot_updrs_progression(updrs_df, patno=None, title_prefix='UPDRS'):\nif updrs_df is None:\nlogging.warning('No UPDRS data passed')\nreturn\ndf = updrs_df.copy()\nif 'PATNO' not in df.columns:\nlogging.error('PATNO not in UPDRS table')\nreturn\nif patno is None:\npatno = df['PATNO'].iloc[0]\n\n\nsel = df[df['PATNO'] == patno].copy()\nif 'INFODT' in sel.columns:\nsel = sel.sort_values('INFODT')\nx = sel['INFODT']\nelse:\nsel = sel.sort_values('EVENT_ID') if 'EVENT_ID' in sel.columns else sel\nx = range(len(sel))\n\n\nif 'TOTAL' not in sel.columns:\nlogging.warning('TOTAL not computed; attempting to compute')\nsel = compute_updrs_total(sel)\n\n\nplt.figure(figsize=(10,4))\nplt.plot(x, sel['TOTAL'], marker='o')\nplt.title(f\"{title_prefix} progression for PATNO {patno}\")\nplt.xlabel('Visit Date' if 'INFODT' in sel.columns else 'Visit Index')\nplt.ylabel('Total Score')\nplt.tight_layout()\nplt.show()\n\n\n# Example plot (will only show if a sample participant exists)\nif updrs1_tot is not None and not updrs1_tot.empty:\nsample_pat = updrs1_tot['PATNO'].iloc[0]\nplot_updrs_progression(updrs1_tot, patno=sample_pat, title_prefix='UPDRS Part I')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "daa7ca0a-99f5-4a27-ad6a-c00b60cefb78",
      "cell_type": "code",
      "source": "# ## 8. Medication (LEDD) processing example\n# The guide suggests creating a table that records LEDD changes over time. We'll provide\n# a helper to build a simple version from the LEDD_Concomitant_Medication_Log table.\n\n\n\n\n\ndef build_ledd_timeline(ledd_df):\n\"\"\"Create a simple timeline of LEDD per PATNO by aggregating LEDD rows by start date.\nNote: the PPMI guide uses month-level dates; this is a conservative example.\n\"\"\"\nif ledd_df is None:\nlogging.warning('LEDD table not available')\nreturn None\ndf = ledd_df.copy()\n\n\n# Ensure STARTDT is parseable\nif 'STARTDT' in df.columns:\ndf['STARTDT'] = pd.to_datetime(df['STARTDT'], errors='coerce')\n# normalize LEDD column name possibilities\nled_cols = [c for c in df.columns if c.upper() in ('LEDD','LEDDSUM','LD')]\nif led_cols:\ndf['LEDD_VAL'] = pd.to_numeric(df[led_cols[0]], errors='coerce')\nelse:\n# attempt to find any numeric column that looks like LEDD\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nif numeric_cols:\ndf['LEDD_VAL'] = df[numeric_cols[0]]\nelse:\nlogging.warning('No LEDD-like column found')\ndf['LEDD_VAL'] = np.nan\n\n\ndf = df.sort_values(['PATNO', 'STARTDT'])\n# keep only rows where LEDD_VAL is present\ndf = df[~df['LEDD_VAL'].isnull()]\n\n\n# For each PATNO, record changes\nout = df.groupby(['PATNO', 'STARTDT'])['LEDD_VAL'].sum().reset_index()\nout = out.sort_values(['PATNO', 'STARTDT'])\nreturn out\n\n\nled_timeline = build_ledd_timeline(LEDD_log)\nshow_basic_info(led_timeline, 'LEDD Timeline Sample')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a538c3fc-32c1-47e7-893f-8ec42425bcac",
      "cell_type": "code",
      "source": "# ## 9. Processed data preparation (all work done in this notebook)\n# 1. Load raw PPMI CSV files from `./PPMI_raw/`.\n# 2. Clean column names, fix date formats, harmonize EVENT_ID, and standardize missing values.\n# 3. Create clinical tables, UPDRS longitudinal tables, and voice/acoustic feature tables directly in Python.\n# 4. Save cleaned versions into `./processed/` so later ML cells read from them.\n#\n# Load any already-processed or partially cleaned files if desired:\n\n\ndef load_processed(name, directory=PROCESSED_DIR):\nfp = find_file(name, directory)\nif fp is None:\nreturn None\nreturn pd.read_csv(fp, low_memory=False)\n\n\nprocessed_clinical = load_processed('clinical_clean')\nprocessed_updrs = load_processed('updrs_longitudinal')\nprocessed_voice = load_processed('voice_features')(name, directory=PROCESSED_DIR):\nfp = find_file(name, directory)\nif fp is None:\nreturn None\nreturn pd.read_csv(fp, low_memory=False)\n\n\nprocessed_clinical = load_processed('clinical_clean')\nprocessed_updrs = load_processed('updrs_longitudinal')\nprocessed_voice = load_processed('voice_features')\n\n\nshow_basic_info(processed_clinical, 'Processed Clinical')\nshow_basic_info(processed_updrs, 'Processed UPDRS')\nshow_basic_info(processed_voice, 'Processed Voice Features')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "30e3b4af-bfe6-456f-a924-50a1577f5293",
      "cell_type": "code",
      "source": "# ## 10. Quick ML-ready checks & train/test split (example using processed_clinical)\n# These cells assume R has produced a tidy clinical table ready for modeling where:\n# - PATNO is present\n# - TARGET is a binary column (1 = PD, 0 = Control)\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\ndef prepare_ml_data(clinical_df, target_col='TARGET', id_col='PATNO', test_size=0.2, random_state=42):\nif clinical_df is None:\nlogging.error('Processed clinical dataframe required for ML prep')\nreturn None\ndf = clinical_df.copy()\nif id_col not in df.columns or target_col not in df.columns:\nlogging.error(f\"Required columns '{id_col}' or '{target_col}' missing in processed clinical data\")\nreturn None\n\n\nX = df.drop(columns=[id_col, target_col])\ny = df[target_col].astype(int)\n\n\n# Simple impute numeric features\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\nimp = SimpleImputer(strategy='median')\nX[num_cols] = imp.fit_transform(X[num_cols])\n\n\n# Scale numeric features\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\nreturn X_train, X_test, y_train, y_test\n\n\n# Example (will do nothing if processed_clinical not present)\nml_split_example = None\nif processed_clinical is not None:\nml_split_example = prepare_ml_data(processed_clinical, target_col='TARGET')\nif ml_split_example is not None:\nX_train, X_test, y_train, y_test = ml_split_example\nprint('ML split shapes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c84d61a0-973b-4d12-bc85-713702c578a2",
      "cell_type": "code",
      "source": "# ## 11. Model skeleton (baseline classifier) — ready to run after processed data provided\n# We'll provide a simple pipeline using RandomForest as a baseline and show where to plug in\n# XAI (SHAP) and robustness tests.\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support, confusion_matrix\n\n\n\n\ndef baseline_train_eval(X_train, X_test, y_train, y_test):\nclf = RandomForestClassifier(n_estimators=200, random_state=0, n_jobs=-1)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, 'predict_proba') else None\n\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\np, r, f, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n\n\nprint('Accuracy:', acc)\nif auc is not None:\nprint('ROC AUC:', auc)\nprint('Precision, Recall, F1:', (p, r, f))\nprint('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\nreturn clf\n\n\n# Example (only run if ml_split_example exists)\nif ml_split_example is not None:\nclf = baseline_train_eval(X_train, X_test, y_train, y_test)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "da85d0c6-5f61-4cfa-a2bc-21c92825e912",
      "cell_type": "code",
      "source": "# ## 12. XAI hooks (SHAP)\n# Example code to compute SHAP values for the RandomForest baseline. Run after model training.\n\n\n\ntry:\nimport shap\nhas_shap = True\nexcept Exception:\nhas_shap = False\nlogging.warning('SHAP not installed; install shap to run explainability analyses')\n\n\nif has_shap and 'clf' in globals() and ml_split_example is not None:\nexplainer = shap.TreeExplainer(clf)\nshap_vals = explainer.shap_values(X_test)\n# For binary classification shap_vals[1] corresponds to positive class\nshap.summary_plot(shap_vals[1], X_test, show=False)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary.png'))\nprint('Saved SHAP summary plot to output folder')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74610165-a1d2-4dff-a7a3-60a32ab6754b",
      "cell_type": "code",
      "source": "# ## 13. Robustness testing placeholder\n# Outline: to test robustness to noise, create noisy variants of audio-derived features or\n# perturb numeric features with gaussian noise / domain shifts and re-evaluate model.\n\n\n\n\n\ndef evaluate_robustness_numeric(clf, X_test, y_test, noise_snr_db=20):\n\"\"\"Add Gaussian noise scaled by desired SNR (dB) to numeric features and eval model.\nThis is a placeholder — for audio-level noise you should perturb audio files before\nextracting features in R or Python audio pipelines.\n\"\"\"\nXn = X_test.copy()\n# Compute signal power and noise power relation\nrms_signal = np.sqrt((Xn ** 2).mean())\n# Convert SNR dB to linear\nsnr_lin = 10 ** (noise_snr_db / 10.0)\nnoise_std = np.sqrt(rms_signal / snr_lin)\nnoise = np.random.normal(0, noise_std, size=Xn.shape)\nXn += noise\ny_pred = clf.predict(Xn)\nacc = accuracy_score(y_test, y_pred)\nprint(f'Robustness test at SNR={noise_snr_db} dB: Accuracy = {acc:.4f}')\n\n\nif ml_split_example is not None and 'clf' in globals():\nevaluate_robustness_numeric(clf, X_test, y_test, noise_snr_db=10)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35e1758a-a43d-4550-b6c4-31af0de43ca1",
      "cell_type": "code",
      "source": "# ## 14. Saving outputs & reproducibility\n# Save participant_master and any processed outputs you produce for traceability.\n\n\n\nif participant_master is not None:\nout_fp = os.path.join(OUTPUT_DIR, 'participant_master_sample.csv')\nparticipant_master.head(200).to_csv(out_fp, index=False)\nprint('Saved participant_master sample to', out_fp)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}